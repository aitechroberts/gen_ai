What is the best number of tokens and the best method of tokenization

Example: lots of versions of Llama done for trying different tokenizers like Llama 2 using BPE

LLM Components: Vocabulary Creation aka word embeddings
1. Tokenization
2. Filtering and Normalization
    - Possibly removing stopwords, process to all lowercase maybe
3. Build Vocabulary
- Collect unique tokens post-toeknization and preprocessing
- Assign each token a unique numerical index for model representation

